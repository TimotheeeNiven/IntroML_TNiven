{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TimotheeeNiven/IntroML_TNiven/blob/main/Homework7_TNiven.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5KtHzyXwb4yH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "import datetime\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9Mc0VICb-fv",
        "outputId": "9b2a5a7c-a77e-4d67-a30c-6c8a5ac072d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 6s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZr1llPhb_A0"
      },
      "outputs": [],
      "source": [
        "# Build CNN model\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))  # Adjust the number of output classes\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNZa30NhcCyi",
        "outputId": "b8ae9142-0442-46d2-b0fa-74484988c795"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "1563/1563 [==============================] - 82s 51ms/step - loss: 1.4926 - accuracy: 0.4557 - val_loss: 1.2482 - val_accuracy: 0.5485\n",
            "Epoch 2/300\n",
            "1563/1563 [==============================] - 76s 49ms/step - loss: 1.1462 - accuracy: 0.5931 - val_loss: 1.0868 - val_accuracy: 0.6165\n",
            "Epoch 3/300\n",
            "1563/1563 [==============================] - 76s 49ms/step - loss: 1.0136 - accuracy: 0.6446 - val_loss: 0.9993 - val_accuracy: 0.6495\n",
            "Epoch 4/300\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.9215 - accuracy: 0.6775 - val_loss: 0.9555 - val_accuracy: 0.6664\n",
            "Epoch 5/300\n",
            "1563/1563 [==============================] - 76s 49ms/step - loss: 0.8441 - accuracy: 0.7046 - val_loss: 0.9160 - val_accuracy: 0.6801\n",
            "Epoch 6/300\n",
            "1563/1563 [==============================] - 76s 49ms/step - loss: 0.7809 - accuracy: 0.7256 - val_loss: 0.8855 - val_accuracy: 0.6926\n",
            "Epoch 7/300\n",
            "1563/1563 [==============================] - 79s 50ms/step - loss: 0.7259 - accuracy: 0.7440 - val_loss: 0.8577 - val_accuracy: 0.7073\n",
            "Epoch 8/300\n",
            "1563/1563 [==============================] - 77s 49ms/step - loss: 0.6810 - accuracy: 0.7619 - val_loss: 0.8688 - val_accuracy: 0.7087\n",
            "Epoch 9/300\n",
            "1563/1563 [==============================] - 74s 48ms/step - loss: 0.6317 - accuracy: 0.7778 - val_loss: 0.8779 - val_accuracy: 0.7079\n",
            "Epoch 10/300\n",
            "1563/1563 [==============================] - 76s 49ms/step - loss: 0.5948 - accuracy: 0.7901 - val_loss: 0.8656 - val_accuracy: 0.7083\n",
            "Epoch 11/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.5528 - accuracy: 0.8047 - val_loss: 0.9263 - val_accuracy: 0.7016\n",
            "Epoch 12/300\n",
            "1563/1563 [==============================] - 73s 46ms/step - loss: 0.5109 - accuracy: 0.8192 - val_loss: 0.9358 - val_accuracy: 0.7001\n",
            "Epoch 13/300\n",
            "1563/1563 [==============================] - 76s 48ms/step - loss: 0.4801 - accuracy: 0.8278 - val_loss: 0.9390 - val_accuracy: 0.7094\n",
            "Epoch 14/300\n",
            "1563/1563 [==============================] - 76s 48ms/step - loss: 0.4351 - accuracy: 0.8445 - val_loss: 0.9944 - val_accuracy: 0.7029\n",
            "Epoch 15/300\n",
            "1563/1563 [==============================] - 76s 48ms/step - loss: 0.4179 - accuracy: 0.8512 - val_loss: 0.9902 - val_accuracy: 0.7032\n",
            "Epoch 16/300\n",
            "1563/1563 [==============================] - 77s 50ms/step - loss: 0.3749 - accuracy: 0.8674 - val_loss: 1.0659 - val_accuracy: 0.7087\n",
            "Epoch 17/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.3455 - accuracy: 0.8771 - val_loss: 1.1643 - val_accuracy: 0.6947\n",
            "Epoch 18/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.3210 - accuracy: 0.8852 - val_loss: 1.1545 - val_accuracy: 0.6975\n",
            "Epoch 19/300\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.2886 - accuracy: 0.8969 - val_loss: 1.2661 - val_accuracy: 0.6986\n",
            "Epoch 20/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.2743 - accuracy: 0.8997 - val_loss: 1.2578 - val_accuracy: 0.7002\n",
            "Epoch 21/300\n",
            "1563/1563 [==============================] - 74s 48ms/step - loss: 0.2599 - accuracy: 0.9068 - val_loss: 1.3989 - val_accuracy: 0.6896\n",
            "Epoch 22/300\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.2312 - accuracy: 0.9165 - val_loss: 1.5122 - val_accuracy: 0.6720\n",
            "Epoch 23/300\n",
            "1563/1563 [==============================] - 76s 49ms/step - loss: 0.2248 - accuracy: 0.9186 - val_loss: 1.5054 - val_accuracy: 0.6863\n",
            "Epoch 24/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.2011 - accuracy: 0.9279 - val_loss: 1.5954 - val_accuracy: 0.6897\n",
            "Epoch 25/300\n",
            "1563/1563 [==============================] - 77s 49ms/step - loss: 0.1925 - accuracy: 0.9306 - val_loss: 1.6469 - val_accuracy: 0.6879\n",
            "Epoch 26/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.1785 - accuracy: 0.9355 - val_loss: 1.7117 - val_accuracy: 0.6869\n",
            "Epoch 27/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.1769 - accuracy: 0.9367 - val_loss: 1.7503 - val_accuracy: 0.6873\n",
            "Epoch 28/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.1695 - accuracy: 0.9408 - val_loss: 1.8778 - val_accuracy: 0.6839\n",
            "Epoch 29/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.1713 - accuracy: 0.9396 - val_loss: 1.8616 - val_accuracy: 0.6942\n",
            "Epoch 30/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.1585 - accuracy: 0.9451 - val_loss: 1.8702 - val_accuracy: 0.6948\n",
            "Epoch 31/300\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.1499 - accuracy: 0.9460 - val_loss: 1.9899 - val_accuracy: 0.6847\n",
            "Epoch 32/300\n",
            "1563/1563 [==============================] - 74s 48ms/step - loss: 0.1478 - accuracy: 0.9484 - val_loss: 2.0548 - val_accuracy: 0.6854\n",
            "Epoch 33/300\n",
            "1563/1563 [==============================] - 74s 48ms/step - loss: 0.1367 - accuracy: 0.9507 - val_loss: 2.1239 - val_accuracy: 0.6889\n",
            "Epoch 34/300\n",
            "1563/1563 [==============================] - 77s 49ms/step - loss: 0.1406 - accuracy: 0.9511 - val_loss: 2.1737 - val_accuracy: 0.6807\n",
            "Epoch 35/300\n",
            "1563/1563 [==============================] - 73s 47ms/step - loss: 0.1381 - accuracy: 0.9530 - val_loss: 2.1693 - val_accuracy: 0.6885\n",
            "Epoch 36/300\n",
            "1563/1563 [==============================] - 76s 48ms/step - loss: 0.1299 - accuracy: 0.9563 - val_loss: 2.2789 - val_accuracy: 0.6817\n",
            "Epoch 37/300\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.1255 - accuracy: 0.9570 - val_loss: 2.2697 - val_accuracy: 0.6829\n",
            "Epoch 38/300\n",
            "1563/1563 [==============================] - 74s 48ms/step - loss: 0.1308 - accuracy: 0.9545 - val_loss: 2.2959 - val_accuracy: 0.6758\n",
            "Epoch 39/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.1201 - accuracy: 0.9591 - val_loss: 2.3706 - val_accuracy: 0.6859\n",
            "Epoch 40/300\n",
            "1563/1563 [==============================] - 77s 49ms/step - loss: 0.1245 - accuracy: 0.9577 - val_loss: 2.5581 - val_accuracy: 0.6772\n",
            "Epoch 41/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.1204 - accuracy: 0.9587 - val_loss: 2.5472 - val_accuracy: 0.6742\n",
            "Epoch 42/300\n",
            "1563/1563 [==============================] - 76s 48ms/step - loss: 0.1182 - accuracy: 0.9608 - val_loss: 2.4151 - val_accuracy: 0.6841\n",
            "Epoch 43/300\n",
            "1563/1563 [==============================] - 77s 49ms/step - loss: 0.1219 - accuracy: 0.9587 - val_loss: 2.6651 - val_accuracy: 0.6787\n",
            "Epoch 44/300\n",
            "1563/1563 [==============================] - 73s 47ms/step - loss: 0.1132 - accuracy: 0.9620 - val_loss: 2.6076 - val_accuracy: 0.6819\n",
            "Epoch 45/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.1207 - accuracy: 0.9598 - val_loss: 2.6150 - val_accuracy: 0.6839\n",
            "Epoch 46/300\n",
            "1563/1563 [==============================] - 76s 49ms/step - loss: 0.1112 - accuracy: 0.9625 - val_loss: 2.6919 - val_accuracy: 0.6807\n",
            "Epoch 47/300\n",
            "1563/1563 [==============================] - 76s 49ms/step - loss: 0.1078 - accuracy: 0.9633 - val_loss: 2.8105 - val_accuracy: 0.6765\n",
            "Epoch 48/300\n",
            "1563/1563 [==============================] - 76s 48ms/step - loss: 0.1143 - accuracy: 0.9633 - val_loss: 2.7181 - val_accuracy: 0.6757\n",
            "Epoch 49/300\n",
            "1563/1563 [==============================] - 76s 49ms/step - loss: 0.1023 - accuracy: 0.9652 - val_loss: 2.7335 - val_accuracy: 0.6805\n",
            "Epoch 50/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.1147 - accuracy: 0.9623 - val_loss: 2.7263 - val_accuracy: 0.6842\n",
            "Epoch 51/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.1037 - accuracy: 0.9664 - val_loss: 2.8929 - val_accuracy: 0.6817\n",
            "Epoch 52/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.1023 - accuracy: 0.9667 - val_loss: 2.9318 - val_accuracy: 0.6791\n",
            "Epoch 53/300\n",
            "1563/1563 [==============================] - 74s 48ms/step - loss: 0.1025 - accuracy: 0.9660 - val_loss: 2.9190 - val_accuracy: 0.6800\n",
            "Epoch 54/300\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.1117 - accuracy: 0.9641 - val_loss: 2.9135 - val_accuracy: 0.6877\n",
            "Epoch 55/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0967 - accuracy: 0.9681 - val_loss: 2.9425 - val_accuracy: 0.6730\n",
            "Epoch 56/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.1014 - accuracy: 0.9676 - val_loss: 2.9787 - val_accuracy: 0.6861\n",
            "Epoch 57/300\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.0972 - accuracy: 0.9675 - val_loss: 3.0994 - val_accuracy: 0.6834\n",
            "Epoch 58/300\n",
            "1563/1563 [==============================] - 74s 48ms/step - loss: 0.0934 - accuracy: 0.9702 - val_loss: 3.1864 - val_accuracy: 0.6767\n",
            "Epoch 59/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.0986 - accuracy: 0.9684 - val_loss: 3.2392 - val_accuracy: 0.6804\n",
            "Epoch 60/300\n",
            "1563/1563 [==============================] - 76s 49ms/step - loss: 0.1002 - accuracy: 0.9674 - val_loss: 3.0809 - val_accuracy: 0.6846\n",
            "Epoch 61/300\n",
            "1563/1563 [==============================] - 79s 51ms/step - loss: 0.1010 - accuracy: 0.9673 - val_loss: 3.2789 - val_accuracy: 0.6833\n",
            "Epoch 62/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.1018 - accuracy: 0.9683 - val_loss: 3.2389 - val_accuracy: 0.6791\n",
            "Epoch 63/300\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.0957 - accuracy: 0.9703 - val_loss: 3.0399 - val_accuracy: 0.6875\n",
            "Epoch 64/300\n",
            "1563/1563 [==============================] - 74s 48ms/step - loss: 0.0987 - accuracy: 0.9695 - val_loss: 3.1994 - val_accuracy: 0.6873\n",
            "Epoch 65/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.0939 - accuracy: 0.9709 - val_loss: 3.2727 - val_accuracy: 0.6821\n",
            "Epoch 66/300\n",
            "1563/1563 [==============================] - 76s 49ms/step - loss: 0.0863 - accuracy: 0.9724 - val_loss: 3.2651 - val_accuracy: 0.6869\n",
            "Epoch 67/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.1032 - accuracy: 0.9685 - val_loss: 3.2509 - val_accuracy: 0.6848\n",
            "Epoch 68/300\n",
            "1563/1563 [==============================] - 76s 48ms/step - loss: 0.0842 - accuracy: 0.9731 - val_loss: 3.2695 - val_accuracy: 0.6815\n",
            "Epoch 69/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0974 - accuracy: 0.9698 - val_loss: 3.4072 - val_accuracy: 0.6740\n",
            "Epoch 70/300\n",
            "1563/1563 [==============================] - 76s 49ms/step - loss: 0.0863 - accuracy: 0.9728 - val_loss: 3.5173 - val_accuracy: 0.6745\n",
            "Epoch 71/300\n",
            "1563/1563 [==============================] - 77s 49ms/step - loss: 0.0919 - accuracy: 0.9723 - val_loss: 3.4476 - val_accuracy: 0.6768\n",
            "Epoch 72/300\n",
            "1563/1563 [==============================] - 76s 49ms/step - loss: 0.0921 - accuracy: 0.9710 - val_loss: 3.4400 - val_accuracy: 0.6757\n",
            "Epoch 73/300\n",
            "1563/1563 [==============================] - 76s 49ms/step - loss: 0.0939 - accuracy: 0.9708 - val_loss: 3.3981 - val_accuracy: 0.6862\n",
            "Epoch 74/300\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.0899 - accuracy: 0.9734 - val_loss: 3.6017 - val_accuracy: 0.6718\n",
            "Epoch 75/300\n",
            "1563/1563 [==============================] - 74s 48ms/step - loss: 0.0873 - accuracy: 0.9729 - val_loss: 3.5313 - val_accuracy: 0.6809\n",
            "Epoch 76/300\n",
            "1563/1563 [==============================] - 76s 49ms/step - loss: 0.0921 - accuracy: 0.9716 - val_loss: 3.6139 - val_accuracy: 0.6831\n",
            "Epoch 77/300\n",
            "1563/1563 [==============================] - 79s 51ms/step - loss: 0.0920 - accuracy: 0.9728 - val_loss: 3.6810 - val_accuracy: 0.6791\n",
            "Epoch 78/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.0839 - accuracy: 0.9744 - val_loss: 3.8034 - val_accuracy: 0.6631\n",
            "Epoch 79/300\n",
            "1563/1563 [==============================] - 82s 52ms/step - loss: 0.0982 - accuracy: 0.9703 - val_loss: 3.5051 - val_accuracy: 0.6823\n",
            "Epoch 80/300\n",
            "1563/1563 [==============================] - 76s 49ms/step - loss: 0.0826 - accuracy: 0.9750 - val_loss: 3.6437 - val_accuracy: 0.6744\n",
            "Epoch 81/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.0844 - accuracy: 0.9742 - val_loss: 3.7678 - val_accuracy: 0.6771\n",
            "Epoch 82/300\n",
            "1563/1563 [==============================] - 79s 50ms/step - loss: 0.0967 - accuracy: 0.9707 - val_loss: 3.5058 - val_accuracy: 0.6830\n",
            "Epoch 83/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.0821 - accuracy: 0.9752 - val_loss: 3.7641 - val_accuracy: 0.6723\n",
            "Epoch 84/300\n",
            "1563/1563 [==============================] - 77s 49ms/step - loss: 0.0904 - accuracy: 0.9729 - val_loss: 3.9057 - val_accuracy: 0.6699\n",
            "Epoch 85/300\n",
            "1563/1563 [==============================] - 79s 50ms/step - loss: 0.0859 - accuracy: 0.9743 - val_loss: 3.9093 - val_accuracy: 0.6756\n",
            "Epoch 86/300\n",
            "1563/1563 [==============================] - 72s 46ms/step - loss: 0.0885 - accuracy: 0.9747 - val_loss: 3.9242 - val_accuracy: 0.6692\n",
            "Epoch 87/300\n",
            "1563/1563 [==============================] - 69s 44ms/step - loss: 0.0856 - accuracy: 0.9741 - val_loss: 3.7531 - val_accuracy: 0.6744\n",
            "Epoch 88/300\n",
            "1563/1563 [==============================] - 71s 46ms/step - loss: 0.0823 - accuracy: 0.9758 - val_loss: 3.9617 - val_accuracy: 0.6788\n",
            "Epoch 89/300\n",
            "1563/1563 [==============================] - 69s 44ms/step - loss: 0.0907 - accuracy: 0.9735 - val_loss: 3.8817 - val_accuracy: 0.6788\n",
            "Epoch 90/300\n",
            "1563/1563 [==============================] - 68s 43ms/step - loss: 0.0775 - accuracy: 0.9770 - val_loss: 3.9162 - val_accuracy: 0.6751\n",
            "Epoch 91/300\n",
            "1563/1563 [==============================] - 71s 45ms/step - loss: 0.0920 - accuracy: 0.9739 - val_loss: 3.8453 - val_accuracy: 0.6814\n",
            "Epoch 92/300\n",
            "1563/1563 [==============================] - 68s 43ms/step - loss: 0.0790 - accuracy: 0.9767 - val_loss: 3.8428 - val_accuracy: 0.6770\n",
            "Epoch 93/300\n",
            "1563/1563 [==============================] - 69s 44ms/step - loss: 0.0824 - accuracy: 0.9765 - val_loss: 4.0384 - val_accuracy: 0.6827\n",
            "Epoch 94/300\n",
            "1563/1563 [==============================] - 69s 44ms/step - loss: 0.0823 - accuracy: 0.9765 - val_loss: 3.9555 - val_accuracy: 0.6788\n",
            "Epoch 95/300\n",
            "1563/1563 [==============================] - 68s 43ms/step - loss: 0.0785 - accuracy: 0.9760 - val_loss: 3.9287 - val_accuracy: 0.6806\n",
            "Epoch 96/300\n",
            "1563/1563 [==============================] - 68s 43ms/step - loss: 0.0871 - accuracy: 0.9749 - val_loss: 3.9994 - val_accuracy: 0.6801\n",
            "Epoch 97/300\n",
            "1563/1563 [==============================] - 69s 44ms/step - loss: 0.0968 - accuracy: 0.9730 - val_loss: 4.0998 - val_accuracy: 0.6765\n",
            "Epoch 98/300\n",
            "1563/1563 [==============================] - 70s 44ms/step - loss: 0.0749 - accuracy: 0.9776 - val_loss: 3.9606 - val_accuracy: 0.6774\n",
            "Epoch 99/300\n",
            "1563/1563 [==============================] - 68s 44ms/step - loss: 0.0848 - accuracy: 0.9758 - val_loss: 4.0372 - val_accuracy: 0.6723\n",
            "Epoch 100/300\n",
            "1563/1563 [==============================] - 68s 43ms/step - loss: 0.0793 - accuracy: 0.9771 - val_loss: 4.1430 - val_accuracy: 0.6751\n",
            "Epoch 101/300\n",
            "1563/1563 [==============================] - 69s 44ms/step - loss: 0.0846 - accuracy: 0.9760 - val_loss: 4.1544 - val_accuracy: 0.6764\n",
            "Epoch 102/300\n",
            "1563/1563 [==============================] - 67s 43ms/step - loss: 0.0786 - accuracy: 0.9773 - val_loss: 4.2559 - val_accuracy: 0.6748\n",
            "Epoch 103/300\n",
            "1563/1563 [==============================] - 69s 44ms/step - loss: 0.0850 - accuracy: 0.9763 - val_loss: 4.1610 - val_accuracy: 0.6825\n",
            "Epoch 104/300\n",
            "1563/1563 [==============================] - 70s 45ms/step - loss: 0.0856 - accuracy: 0.9763 - val_loss: 4.1406 - val_accuracy: 0.6753\n",
            "Epoch 105/300\n",
            "1563/1563 [==============================] - 70s 45ms/step - loss: 0.0843 - accuracy: 0.9765 - val_loss: 4.3303 - val_accuracy: 0.6740\n",
            "Epoch 106/300\n",
            "1563/1563 [==============================] - 70s 45ms/step - loss: 0.0760 - accuracy: 0.9781 - val_loss: 4.3185 - val_accuracy: 0.6802\n",
            "Epoch 107/300\n",
            "1563/1563 [==============================] - 70s 45ms/step - loss: 0.0818 - accuracy: 0.9772 - val_loss: 4.2880 - val_accuracy: 0.6700\n",
            "Epoch 108/300\n",
            "1563/1563 [==============================] - 68s 44ms/step - loss: 0.0919 - accuracy: 0.9753 - val_loss: 4.2846 - val_accuracy: 0.6818\n",
            "Epoch 109/300\n",
            "1563/1563 [==============================] - 70s 45ms/step - loss: 0.0742 - accuracy: 0.9792 - val_loss: 4.3122 - val_accuracy: 0.6808\n",
            "Epoch 110/300\n",
            "1563/1563 [==============================] - 71s 46ms/step - loss: 0.0891 - accuracy: 0.9759 - val_loss: 4.2933 - val_accuracy: 0.6806\n",
            "Epoch 111/300\n",
            "1563/1563 [==============================] - 71s 45ms/step - loss: 0.0720 - accuracy: 0.9797 - val_loss: 4.4235 - val_accuracy: 0.6791\n",
            "Epoch 112/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0918 - accuracy: 0.9747 - val_loss: 4.2903 - val_accuracy: 0.6733\n",
            "Epoch 113/300\n",
            "1563/1563 [==============================] - 76s 49ms/step - loss: 0.0717 - accuracy: 0.9785 - val_loss: 4.4232 - val_accuracy: 0.6790\n",
            "Epoch 114/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0907 - accuracy: 0.9762 - val_loss: 4.5431 - val_accuracy: 0.6788\n",
            "Epoch 115/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0785 - accuracy: 0.9787 - val_loss: 4.4372 - val_accuracy: 0.6760\n",
            "Epoch 116/300\n",
            "1563/1563 [==============================] - 73s 47ms/step - loss: 0.0878 - accuracy: 0.9763 - val_loss: 4.5711 - val_accuracy: 0.6775\n",
            "Epoch 117/300\n",
            "1563/1563 [==============================] - 73s 47ms/step - loss: 0.0751 - accuracy: 0.9788 - val_loss: 4.5472 - val_accuracy: 0.6788\n",
            "Epoch 118/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0761 - accuracy: 0.9796 - val_loss: 4.6278 - val_accuracy: 0.6733\n",
            "Epoch 119/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.0866 - accuracy: 0.9776 - val_loss: 4.4993 - val_accuracy: 0.6779\n",
            "Epoch 120/300\n",
            "1563/1563 [==============================] - 73s 47ms/step - loss: 0.0791 - accuracy: 0.9784 - val_loss: 4.5069 - val_accuracy: 0.6726\n",
            "Epoch 121/300\n",
            "1563/1563 [==============================] - 73s 47ms/step - loss: 0.0778 - accuracy: 0.9790 - val_loss: 4.7342 - val_accuracy: 0.6722\n",
            "Epoch 122/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.0701 - accuracy: 0.9805 - val_loss: 4.6459 - val_accuracy: 0.6694\n",
            "Epoch 123/300\n",
            "1563/1563 [==============================] - 74s 48ms/step - loss: 0.0878 - accuracy: 0.9773 - val_loss: 4.6109 - val_accuracy: 0.6806\n",
            "Epoch 124/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0842 - accuracy: 0.9786 - val_loss: 4.7648 - val_accuracy: 0.6718\n",
            "Epoch 125/300\n",
            "1563/1563 [==============================] - 74s 48ms/step - loss: 0.0743 - accuracy: 0.9806 - val_loss: 4.8687 - val_accuracy: 0.6705\n",
            "Epoch 126/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.0857 - accuracy: 0.9778 - val_loss: 4.8436 - val_accuracy: 0.6715\n",
            "Epoch 127/300\n",
            "1563/1563 [==============================] - 72s 46ms/step - loss: 0.0859 - accuracy: 0.9776 - val_loss: 4.6792 - val_accuracy: 0.6643\n",
            "Epoch 128/300\n",
            "1563/1563 [==============================] - 72s 46ms/step - loss: 0.0755 - accuracy: 0.9805 - val_loss: 4.8886 - val_accuracy: 0.6670\n",
            "Epoch 129/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.0852 - accuracy: 0.9785 - val_loss: 4.6688 - val_accuracy: 0.6739\n",
            "Epoch 130/300\n",
            "1563/1563 [==============================] - 73s 47ms/step - loss: 0.0807 - accuracy: 0.9792 - val_loss: 4.8366 - val_accuracy: 0.6829\n",
            "Epoch 131/300\n",
            "1563/1563 [==============================] - 72s 46ms/step - loss: 0.0711 - accuracy: 0.9807 - val_loss: 4.9701 - val_accuracy: 0.6769\n",
            "Epoch 132/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.0892 - accuracy: 0.9769 - val_loss: 4.9271 - val_accuracy: 0.6724\n",
            "Epoch 133/300\n",
            "1563/1563 [==============================] - 73s 47ms/step - loss: 0.0754 - accuracy: 0.9811 - val_loss: 5.0165 - val_accuracy: 0.6698\n",
            "Epoch 134/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0779 - accuracy: 0.9795 - val_loss: 4.9073 - val_accuracy: 0.6764\n",
            "Epoch 135/300\n",
            "1563/1563 [==============================] - 76s 49ms/step - loss: 0.0851 - accuracy: 0.9784 - val_loss: 5.1156 - val_accuracy: 0.6751\n",
            "Epoch 136/300\n",
            "1563/1563 [==============================] - 72s 46ms/step - loss: 0.0723 - accuracy: 0.9817 - val_loss: 4.9967 - val_accuracy: 0.6788\n",
            "Epoch 137/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0868 - accuracy: 0.9782 - val_loss: 5.1037 - val_accuracy: 0.6675\n",
            "Epoch 138/300\n",
            "1563/1563 [==============================] - 76s 48ms/step - loss: 0.0727 - accuracy: 0.9815 - val_loss: 5.0669 - val_accuracy: 0.6765\n",
            "Epoch 139/300\n",
            "1563/1563 [==============================] - 73s 47ms/step - loss: 0.0766 - accuracy: 0.9801 - val_loss: 5.1007 - val_accuracy: 0.6758\n",
            "Epoch 140/300\n",
            "1563/1563 [==============================] - 71s 46ms/step - loss: 0.0798 - accuracy: 0.9801 - val_loss: 5.2214 - val_accuracy: 0.6720\n",
            "Epoch 141/300\n",
            "1563/1563 [==============================] - 73s 47ms/step - loss: 0.0823 - accuracy: 0.9789 - val_loss: 5.2435 - val_accuracy: 0.6783\n",
            "Epoch 142/300\n",
            "1563/1563 [==============================] - 73s 46ms/step - loss: 0.0761 - accuracy: 0.9799 - val_loss: 5.1021 - val_accuracy: 0.6801\n",
            "Epoch 143/300\n",
            "1563/1563 [==============================] - 73s 47ms/step - loss: 0.0790 - accuracy: 0.9803 - val_loss: 5.2330 - val_accuracy: 0.6794\n",
            "Epoch 144/300\n",
            "1563/1563 [==============================] - 73s 47ms/step - loss: 0.0818 - accuracy: 0.9803 - val_loss: 5.3539 - val_accuracy: 0.6778\n",
            "Epoch 145/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.0870 - accuracy: 0.9783 - val_loss: 5.4007 - val_accuracy: 0.6740\n",
            "Epoch 146/300\n",
            "1563/1563 [==============================] - 72s 46ms/step - loss: 0.0693 - accuracy: 0.9827 - val_loss: 5.4124 - val_accuracy: 0.6849\n",
            "Epoch 147/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0903 - accuracy: 0.9786 - val_loss: 5.3442 - val_accuracy: 0.6767\n",
            "Epoch 148/300\n",
            "1563/1563 [==============================] - 74s 48ms/step - loss: 0.0784 - accuracy: 0.9810 - val_loss: 5.6678 - val_accuracy: 0.6712\n",
            "Epoch 149/300\n",
            "1563/1563 [==============================] - 73s 47ms/step - loss: 0.0840 - accuracy: 0.9796 - val_loss: 5.5512 - val_accuracy: 0.6693\n",
            "Epoch 150/300\n",
            "1563/1563 [==============================] - 72s 46ms/step - loss: 0.0778 - accuracy: 0.9807 - val_loss: 5.6198 - val_accuracy: 0.6755\n",
            "Epoch 151/300\n",
            "1563/1563 [==============================] - 76s 48ms/step - loss: 0.0735 - accuracy: 0.9823 - val_loss: 5.4734 - val_accuracy: 0.6730\n",
            "Epoch 152/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0820 - accuracy: 0.9798 - val_loss: 5.5283 - val_accuracy: 0.6724\n",
            "Epoch 153/300\n",
            "1563/1563 [==============================] - 72s 46ms/step - loss: 0.0760 - accuracy: 0.9812 - val_loss: 5.6050 - val_accuracy: 0.6745\n",
            "Epoch 154/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0894 - accuracy: 0.9792 - val_loss: 5.4343 - val_accuracy: 0.6738\n",
            "Epoch 155/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0763 - accuracy: 0.9812 - val_loss: 5.7030 - val_accuracy: 0.6715\n",
            "Epoch 156/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0844 - accuracy: 0.9805 - val_loss: 5.4942 - val_accuracy: 0.6839\n",
            "Epoch 157/300\n",
            "1563/1563 [==============================] - 73s 47ms/step - loss: 0.0771 - accuracy: 0.9807 - val_loss: 5.8182 - val_accuracy: 0.6677\n",
            "Epoch 158/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.0782 - accuracy: 0.9821 - val_loss: 5.7518 - val_accuracy: 0.6787\n",
            "Epoch 159/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.0918 - accuracy: 0.9792 - val_loss: 5.7790 - val_accuracy: 0.6779\n",
            "Epoch 160/300\n",
            "1563/1563 [==============================] - 73s 46ms/step - loss: 0.0774 - accuracy: 0.9818 - val_loss: 5.8201 - val_accuracy: 0.6729\n",
            "Epoch 161/300\n",
            "1563/1563 [==============================] - 76s 49ms/step - loss: 0.0868 - accuracy: 0.9798 - val_loss: 5.6216 - val_accuracy: 0.6813\n",
            "Epoch 162/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.0715 - accuracy: 0.9832 - val_loss: 6.0257 - val_accuracy: 0.6722\n",
            "Epoch 163/300\n",
            "1563/1563 [==============================] - 72s 46ms/step - loss: 0.0893 - accuracy: 0.9797 - val_loss: 5.7710 - val_accuracy: 0.6764\n",
            "Epoch 164/300\n",
            "1563/1563 [==============================] - 73s 47ms/step - loss: 0.0852 - accuracy: 0.9807 - val_loss: 5.7862 - val_accuracy: 0.6742\n",
            "Epoch 165/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0838 - accuracy: 0.9803 - val_loss: 5.9317 - val_accuracy: 0.6795\n",
            "Epoch 166/300\n",
            "1563/1563 [==============================] - 73s 47ms/step - loss: 0.0793 - accuracy: 0.9818 - val_loss: 5.8189 - val_accuracy: 0.6752\n",
            "Epoch 167/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0838 - accuracy: 0.9807 - val_loss: 5.9055 - val_accuracy: 0.6761\n",
            "Epoch 168/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.0877 - accuracy: 0.9810 - val_loss: 5.9825 - val_accuracy: 0.6741\n",
            "Epoch 169/300\n",
            "1563/1563 [==============================] - 73s 47ms/step - loss: 0.0829 - accuracy: 0.9818 - val_loss: 6.2958 - val_accuracy: 0.6720\n",
            "Epoch 170/300\n",
            "1563/1563 [==============================] - 71s 46ms/step - loss: 0.0797 - accuracy: 0.9811 - val_loss: 5.9310 - val_accuracy: 0.6749\n",
            "Epoch 171/300\n",
            "1563/1563 [==============================] - 73s 47ms/step - loss: 0.0757 - accuracy: 0.9832 - val_loss: 5.9155 - val_accuracy: 0.6714\n",
            "Epoch 172/300\n",
            "1563/1563 [==============================] - 73s 47ms/step - loss: 0.0770 - accuracy: 0.9825 - val_loss: 6.1920 - val_accuracy: 0.6723\n",
            "Epoch 173/300\n",
            "1563/1563 [==============================] - 72s 46ms/step - loss: 0.0841 - accuracy: 0.9811 - val_loss: 6.3587 - val_accuracy: 0.6650\n",
            "Epoch 174/300\n",
            "1563/1563 [==============================] - 72s 46ms/step - loss: 0.0813 - accuracy: 0.9812 - val_loss: 6.3303 - val_accuracy: 0.6694\n",
            "Epoch 175/300\n",
            "1563/1563 [==============================] - 76s 48ms/step - loss: 0.0844 - accuracy: 0.9812 - val_loss: 6.4074 - val_accuracy: 0.6728\n",
            "Epoch 176/300\n",
            "1563/1563 [==============================] - 74s 48ms/step - loss: 0.0809 - accuracy: 0.9823 - val_loss: 6.3326 - val_accuracy: 0.6770\n",
            "Epoch 177/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0730 - accuracy: 0.9827 - val_loss: 6.1774 - val_accuracy: 0.6774\n",
            "Epoch 178/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0874 - accuracy: 0.9818 - val_loss: 6.2375 - val_accuracy: 0.6757\n",
            "Epoch 179/300\n",
            "1563/1563 [==============================] - 73s 47ms/step - loss: 0.0796 - accuracy: 0.9828 - val_loss: 6.2749 - val_accuracy: 0.6729\n",
            "Epoch 180/300\n",
            "1563/1563 [==============================] - 73s 47ms/step - loss: 0.0827 - accuracy: 0.9820 - val_loss: 6.4249 - val_accuracy: 0.6733\n",
            "Epoch 181/300\n",
            "1563/1563 [==============================] - 72s 46ms/step - loss: 0.0830 - accuracy: 0.9820 - val_loss: 6.5795 - val_accuracy: 0.6673\n",
            "Epoch 182/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.0814 - accuracy: 0.9824 - val_loss: 6.5130 - val_accuracy: 0.6756\n",
            "Epoch 183/300\n",
            "1563/1563 [==============================] - 71s 46ms/step - loss: 0.0878 - accuracy: 0.9817 - val_loss: 6.6034 - val_accuracy: 0.6754\n",
            "Epoch 184/300\n",
            "1563/1563 [==============================] - 73s 47ms/step - loss: 0.0846 - accuracy: 0.9819 - val_loss: 6.4498 - val_accuracy: 0.6746\n",
            "Epoch 185/300\n",
            "1563/1563 [==============================] - 73s 47ms/step - loss: 0.0884 - accuracy: 0.9812 - val_loss: 6.7605 - val_accuracy: 0.6702\n",
            "Epoch 186/300\n",
            "1563/1563 [==============================] - 73s 47ms/step - loss: 0.0738 - accuracy: 0.9841 - val_loss: 6.6153 - val_accuracy: 0.6686\n",
            "Epoch 187/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0895 - accuracy: 0.9817 - val_loss: 6.6839 - val_accuracy: 0.6771\n",
            "Epoch 188/300\n",
            "1563/1563 [==============================] - 72s 46ms/step - loss: 0.0866 - accuracy: 0.9811 - val_loss: 6.5331 - val_accuracy: 0.6751\n",
            "Epoch 189/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.0805 - accuracy: 0.9813 - val_loss: 6.2201 - val_accuracy: 0.6776\n",
            "Epoch 190/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0847 - accuracy: 0.9820 - val_loss: 6.5571 - val_accuracy: 0.6719\n",
            "Epoch 191/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0857 - accuracy: 0.9831 - val_loss: 6.9708 - val_accuracy: 0.6701\n",
            "Epoch 192/300\n",
            "1563/1563 [==============================] - 73s 47ms/step - loss: 0.0722 - accuracy: 0.9842 - val_loss: 7.0187 - val_accuracy: 0.6708\n",
            "Epoch 193/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0854 - accuracy: 0.9827 - val_loss: 6.9871 - val_accuracy: 0.6769\n",
            "Epoch 194/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0859 - accuracy: 0.9818 - val_loss: 7.0905 - val_accuracy: 0.6761\n",
            "Epoch 195/300\n",
            "1563/1563 [==============================] - 73s 46ms/step - loss: 0.0850 - accuracy: 0.9824 - val_loss: 6.9679 - val_accuracy: 0.6810\n",
            "Epoch 196/300\n",
            "1563/1563 [==============================] - 76s 49ms/step - loss: 0.0831 - accuracy: 0.9845 - val_loss: 7.2567 - val_accuracy: 0.6713\n",
            "Epoch 197/300\n",
            "1563/1563 [==============================] - 74s 48ms/step - loss: 0.0834 - accuracy: 0.9833 - val_loss: 6.9792 - val_accuracy: 0.6765\n",
            "Epoch 198/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0853 - accuracy: 0.9824 - val_loss: 6.9945 - val_accuracy: 0.6709\n",
            "Epoch 199/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0747 - accuracy: 0.9846 - val_loss: 7.2102 - val_accuracy: 0.6736\n",
            "Epoch 200/300\n",
            "1563/1563 [==============================] - 74s 48ms/step - loss: 0.0878 - accuracy: 0.9825 - val_loss: 7.2622 - val_accuracy: 0.6790\n",
            "Epoch 201/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0818 - accuracy: 0.9828 - val_loss: 7.0915 - val_accuracy: 0.6720\n",
            "Epoch 202/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0909 - accuracy: 0.9820 - val_loss: 7.1033 - val_accuracy: 0.6725\n",
            "Epoch 203/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0848 - accuracy: 0.9829 - val_loss: 7.2503 - val_accuracy: 0.6737\n",
            "Epoch 204/300\n",
            "1563/1563 [==============================] - 73s 47ms/step - loss: 0.0882 - accuracy: 0.9824 - val_loss: 7.4368 - val_accuracy: 0.6704\n",
            "Epoch 205/300\n",
            "1563/1563 [==============================] - 72s 46ms/step - loss: 0.0764 - accuracy: 0.9839 - val_loss: 7.7418 - val_accuracy: 0.6687\n",
            "Epoch 206/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0958 - accuracy: 0.9822 - val_loss: 7.4230 - val_accuracy: 0.6623\n",
            "Epoch 207/300\n",
            "1563/1563 [==============================] - 73s 47ms/step - loss: 0.0950 - accuracy: 0.9818 - val_loss: 7.1759 - val_accuracy: 0.6690\n",
            "Epoch 208/300\n",
            "1563/1563 [==============================] - 72s 46ms/step - loss: 0.0991 - accuracy: 0.9811 - val_loss: 7.3300 - val_accuracy: 0.6719\n",
            "Epoch 209/300\n",
            "1563/1563 [==============================] - 73s 47ms/step - loss: 0.0720 - accuracy: 0.9856 - val_loss: 7.5811 - val_accuracy: 0.6777\n",
            "Epoch 210/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.0937 - accuracy: 0.9821 - val_loss: 7.9822 - val_accuracy: 0.6706\n",
            "Epoch 211/300\n",
            "1563/1563 [==============================] - 76s 48ms/step - loss: 0.0784 - accuracy: 0.9837 - val_loss: 7.4471 - val_accuracy: 0.6726\n",
            "Epoch 212/300\n",
            "1563/1563 [==============================] - 73s 46ms/step - loss: 0.0803 - accuracy: 0.9844 - val_loss: 7.7751 - val_accuracy: 0.6763\n",
            "Epoch 213/300\n",
            "1563/1563 [==============================] - 72s 46ms/step - loss: 0.0928 - accuracy: 0.9827 - val_loss: 7.4640 - val_accuracy: 0.6714\n",
            "Epoch 214/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0908 - accuracy: 0.9828 - val_loss: 7.7771 - val_accuracy: 0.6789\n",
            "Epoch 215/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0855 - accuracy: 0.9838 - val_loss: 7.6882 - val_accuracy: 0.6745\n",
            "Epoch 216/300\n",
            "1563/1563 [==============================] - 72s 46ms/step - loss: 0.0845 - accuracy: 0.9834 - val_loss: 7.7339 - val_accuracy: 0.6717\n",
            "Epoch 217/300\n",
            "1563/1563 [==============================] - 76s 49ms/step - loss: 0.0853 - accuracy: 0.9839 - val_loss: 7.6694 - val_accuracy: 0.6756\n",
            "Epoch 218/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0936 - accuracy: 0.9828 - val_loss: 7.8150 - val_accuracy: 0.6787\n",
            "Epoch 219/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0793 - accuracy: 0.9848 - val_loss: 8.2465 - val_accuracy: 0.6752\n",
            "Epoch 220/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0943 - accuracy: 0.9829 - val_loss: 7.8778 - val_accuracy: 0.6760\n",
            "Epoch 221/300\n",
            "1563/1563 [==============================] - 74s 48ms/step - loss: 0.0863 - accuracy: 0.9839 - val_loss: 7.7593 - val_accuracy: 0.6717\n",
            "Epoch 222/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.0754 - accuracy: 0.9847 - val_loss: 8.5181 - val_accuracy: 0.6609\n",
            "Epoch 223/300\n",
            "1563/1563 [==============================] - 73s 46ms/step - loss: 0.0886 - accuracy: 0.9827 - val_loss: 8.1787 - val_accuracy: 0.6666\n",
            "Epoch 224/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0926 - accuracy: 0.9831 - val_loss: 8.0868 - val_accuracy: 0.6755\n",
            "Epoch 225/300\n",
            "1563/1563 [==============================] - 73s 46ms/step - loss: 0.1002 - accuracy: 0.9821 - val_loss: 8.4045 - val_accuracy: 0.6713\n",
            "Epoch 226/300\n",
            "1563/1563 [==============================] - 76s 49ms/step - loss: 0.0849 - accuracy: 0.9838 - val_loss: 8.3156 - val_accuracy: 0.6726\n",
            "Epoch 227/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.0808 - accuracy: 0.9849 - val_loss: 8.2776 - val_accuracy: 0.6778\n",
            "Epoch 228/300\n",
            "1563/1563 [==============================] - 74s 48ms/step - loss: 0.0968 - accuracy: 0.9831 - val_loss: 8.1812 - val_accuracy: 0.6783\n",
            "Epoch 229/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.0851 - accuracy: 0.9839 - val_loss: 8.6379 - val_accuracy: 0.6757\n",
            "Epoch 230/300\n",
            "1563/1563 [==============================] - 73s 46ms/step - loss: 0.0992 - accuracy: 0.9822 - val_loss: 8.7383 - val_accuracy: 0.6687\n",
            "Epoch 231/300\n",
            "1563/1563 [==============================] - 73s 47ms/step - loss: 0.0850 - accuracy: 0.9843 - val_loss: 8.3633 - val_accuracy: 0.6792\n",
            "Epoch 232/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.0873 - accuracy: 0.9840 - val_loss: 8.5251 - val_accuracy: 0.6696\n",
            "Epoch 233/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0911 - accuracy: 0.9849 - val_loss: 9.0866 - val_accuracy: 0.6723\n",
            "Epoch 234/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.1055 - accuracy: 0.9820 - val_loss: 8.5487 - val_accuracy: 0.6721\n",
            "Epoch 235/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0885 - accuracy: 0.9845 - val_loss: 8.3050 - val_accuracy: 0.6740\n",
            "Epoch 236/300\n",
            "1563/1563 [==============================] - 72s 46ms/step - loss: 0.0986 - accuracy: 0.9831 - val_loss: 8.6885 - val_accuracy: 0.6777\n",
            "Epoch 237/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.0838 - accuracy: 0.9848 - val_loss: 8.6302 - val_accuracy: 0.6754\n",
            "Epoch 238/300\n",
            "1563/1563 [==============================] - 72s 46ms/step - loss: 0.0952 - accuracy: 0.9832 - val_loss: 9.0530 - val_accuracy: 0.6713\n",
            "Epoch 239/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0810 - accuracy: 0.9853 - val_loss: 8.9878 - val_accuracy: 0.6676\n",
            "Epoch 240/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.0944 - accuracy: 0.9832 - val_loss: 8.8375 - val_accuracy: 0.6717\n",
            "Epoch 241/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0971 - accuracy: 0.9834 - val_loss: 8.7373 - val_accuracy: 0.6732\n",
            "Epoch 242/300\n",
            "1563/1563 [==============================] - 74s 48ms/step - loss: 0.0939 - accuracy: 0.9834 - val_loss: 8.6702 - val_accuracy: 0.6804\n",
            "Epoch 243/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.0906 - accuracy: 0.9848 - val_loss: 9.0701 - val_accuracy: 0.6743\n",
            "Epoch 244/300\n",
            "1563/1563 [==============================] - 76s 48ms/step - loss: 0.0904 - accuracy: 0.9844 - val_loss: 8.7827 - val_accuracy: 0.6747\n",
            "Epoch 245/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0878 - accuracy: 0.9847 - val_loss: 9.2625 - val_accuracy: 0.6682\n",
            "Epoch 246/300\n",
            "1563/1563 [==============================] - 73s 47ms/step - loss: 0.1018 - accuracy: 0.9835 - val_loss: 9.0728 - val_accuracy: 0.6695\n",
            "Epoch 247/300\n",
            "1563/1563 [==============================] - 70s 45ms/step - loss: 0.0867 - accuracy: 0.9854 - val_loss: 9.1195 - val_accuracy: 0.6709\n",
            "Epoch 248/300\n",
            "1563/1563 [==============================] - 71s 45ms/step - loss: 0.0952 - accuracy: 0.9838 - val_loss: 9.3518 - val_accuracy: 0.6769\n",
            "Epoch 249/300\n",
            "1563/1563 [==============================] - 71s 46ms/step - loss: 0.0970 - accuracy: 0.9843 - val_loss: 9.5315 - val_accuracy: 0.6727\n",
            "Epoch 250/300\n",
            "1563/1563 [==============================] - 73s 47ms/step - loss: 0.1007 - accuracy: 0.9836 - val_loss: 9.1276 - val_accuracy: 0.6667\n",
            "Epoch 251/300\n",
            "1563/1563 [==============================] - 73s 47ms/step - loss: 0.0849 - accuracy: 0.9855 - val_loss: 9.3391 - val_accuracy: 0.6678\n",
            "Epoch 252/300\n",
            "1563/1563 [==============================] - 77s 49ms/step - loss: 0.0967 - accuracy: 0.9833 - val_loss: 9.2736 - val_accuracy: 0.6767\n",
            "Epoch 253/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.0886 - accuracy: 0.9847 - val_loss: 9.5103 - val_accuracy: 0.6735\n",
            "Epoch 254/300\n",
            "1563/1563 [==============================] - 76s 48ms/step - loss: 0.1061 - accuracy: 0.9839 - val_loss: 9.2760 - val_accuracy: 0.6724\n",
            "Epoch 255/300\n",
            "1563/1563 [==============================] - 77s 49ms/step - loss: 0.0883 - accuracy: 0.9856 - val_loss: 9.9985 - val_accuracy: 0.6715\n",
            "Epoch 256/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0973 - accuracy: 0.9846 - val_loss: 9.2915 - val_accuracy: 0.6750\n",
            "Epoch 257/300\n",
            "1563/1563 [==============================] - 76s 49ms/step - loss: 0.1058 - accuracy: 0.9836 - val_loss: 9.8654 - val_accuracy: 0.6659\n",
            "Epoch 258/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.0891 - accuracy: 0.9852 - val_loss: 9.4020 - val_accuracy: 0.6701\n",
            "Epoch 259/300\n",
            "1563/1563 [==============================] - 77s 49ms/step - loss: 0.0872 - accuracy: 0.9856 - val_loss: 9.7066 - val_accuracy: 0.6621\n",
            "Epoch 260/300\n",
            "1563/1563 [==============================] - 76s 49ms/step - loss: 0.0993 - accuracy: 0.9846 - val_loss: 9.6681 - val_accuracy: 0.6722\n",
            "Epoch 261/300\n",
            "1563/1563 [==============================] - 76s 49ms/step - loss: 0.0995 - accuracy: 0.9844 - val_loss: 9.9459 - val_accuracy: 0.6724\n",
            "Epoch 262/300\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.1035 - accuracy: 0.9837 - val_loss: 9.9503 - val_accuracy: 0.6723\n",
            "Epoch 263/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.0948 - accuracy: 0.9855 - val_loss: 10.1406 - val_accuracy: 0.6765\n",
            "Epoch 264/300\n",
            "1563/1563 [==============================] - 77s 49ms/step - loss: 0.0989 - accuracy: 0.9846 - val_loss: 10.4459 - val_accuracy: 0.6741\n",
            "Epoch 265/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.0996 - accuracy: 0.9846 - val_loss: 10.0119 - val_accuracy: 0.6769\n",
            "Epoch 266/300\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.0900 - accuracy: 0.9856 - val_loss: 10.1512 - val_accuracy: 0.6802\n",
            "Epoch 267/300\n",
            "1563/1563 [==============================] - 76s 49ms/step - loss: 0.0946 - accuracy: 0.9856 - val_loss: 10.1075 - val_accuracy: 0.6763\n",
            "Epoch 268/300\n",
            "1563/1563 [==============================] - 76s 49ms/step - loss: 0.1060 - accuracy: 0.9840 - val_loss: 10.1946 - val_accuracy: 0.6711\n",
            "Epoch 269/300\n",
            "1563/1563 [==============================] - 74s 48ms/step - loss: 0.1009 - accuracy: 0.9845 - val_loss: 10.1167 - val_accuracy: 0.6716\n",
            "Epoch 270/300\n",
            "1563/1563 [==============================] - 77s 49ms/step - loss: 0.1029 - accuracy: 0.9849 - val_loss: 10.5489 - val_accuracy: 0.6653\n",
            "Epoch 271/300\n",
            "1563/1563 [==============================] - 76s 49ms/step - loss: 0.0928 - accuracy: 0.9856 - val_loss: 10.6645 - val_accuracy: 0.6701\n",
            "Epoch 272/300\n",
            "1563/1563 [==============================] - 76s 48ms/step - loss: 0.1205 - accuracy: 0.9831 - val_loss: 10.4011 - val_accuracy: 0.6698\n",
            "Epoch 273/300\n",
            "1563/1563 [==============================] - 77s 49ms/step - loss: 0.1001 - accuracy: 0.9844 - val_loss: 11.1610 - val_accuracy: 0.6690\n",
            "Epoch 274/300\n",
            "1563/1563 [==============================] - 76s 49ms/step - loss: 0.0981 - accuracy: 0.9862 - val_loss: 10.6962 - val_accuracy: 0.6654\n",
            "Epoch 275/300\n",
            "1563/1563 [==============================] - 77s 49ms/step - loss: 0.0868 - accuracy: 0.9863 - val_loss: 10.4694 - val_accuracy: 0.6751\n",
            "Epoch 276/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.1074 - accuracy: 0.9843 - val_loss: 10.8501 - val_accuracy: 0.6599\n",
            "Epoch 277/300\n",
            "1563/1563 [==============================] - 77s 49ms/step - loss: 0.1015 - accuracy: 0.9847 - val_loss: 10.7413 - val_accuracy: 0.6714\n",
            "Epoch 278/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.1027 - accuracy: 0.9849 - val_loss: 10.6247 - val_accuracy: 0.6695\n",
            "Epoch 279/300\n",
            "1563/1563 [==============================] - 76s 48ms/step - loss: 0.0748 - accuracy: 0.9875 - val_loss: 10.6314 - val_accuracy: 0.6756\n",
            "Epoch 280/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.1160 - accuracy: 0.9826 - val_loss: 11.0191 - val_accuracy: 0.6677\n",
            "Epoch 281/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.1066 - accuracy: 0.9843 - val_loss: 10.9877 - val_accuracy: 0.6687\n",
            "Epoch 282/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.0951 - accuracy: 0.9863 - val_loss: 10.9627 - val_accuracy: 0.6702\n",
            "Epoch 283/300\n",
            "1563/1563 [==============================] - 76s 48ms/step - loss: 0.1022 - accuracy: 0.9847 - val_loss: 11.0725 - val_accuracy: 0.6673\n",
            "Epoch 284/300\n",
            "1563/1563 [==============================] - 77s 49ms/step - loss: 0.1130 - accuracy: 0.9845 - val_loss: 11.1889 - val_accuracy: 0.6696\n",
            "Epoch 285/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.1040 - accuracy: 0.9851 - val_loss: 11.0335 - val_accuracy: 0.6678\n",
            "Epoch 286/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.1052 - accuracy: 0.9856 - val_loss: 11.3339 - val_accuracy: 0.6656\n",
            "Epoch 287/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.1028 - accuracy: 0.9853 - val_loss: 11.3587 - val_accuracy: 0.6702\n",
            "Epoch 288/300\n",
            "1563/1563 [==============================] - 77s 49ms/step - loss: 0.1158 - accuracy: 0.9842 - val_loss: 10.7940 - val_accuracy: 0.6706\n",
            "Epoch 289/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.0992 - accuracy: 0.9855 - val_loss: 10.7553 - val_accuracy: 0.6646\n",
            "Epoch 290/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.1019 - accuracy: 0.9855 - val_loss: 11.1255 - val_accuracy: 0.6625\n",
            "Epoch 291/300\n",
            "1563/1563 [==============================] - 74s 47ms/step - loss: 0.1061 - accuracy: 0.9850 - val_loss: 11.5102 - val_accuracy: 0.6763\n",
            "Epoch 292/300\n",
            "1563/1563 [==============================] - 76s 49ms/step - loss: 0.0997 - accuracy: 0.9854 - val_loss: 11.7224 - val_accuracy: 0.6776\n",
            "Epoch 293/300\n",
            "1563/1563 [==============================] - 76s 48ms/step - loss: 0.1138 - accuracy: 0.9841 - val_loss: 11.7615 - val_accuracy: 0.6714\n",
            "Epoch 294/300\n",
            "1563/1563 [==============================] - 76s 48ms/step - loss: 0.0980 - accuracy: 0.9860 - val_loss: 11.3252 - val_accuracy: 0.6706\n",
            "Epoch 295/300\n",
            "1563/1563 [==============================] - 74s 48ms/step - loss: 0.0989 - accuracy: 0.9857 - val_loss: 11.7375 - val_accuracy: 0.6751\n",
            "Epoch 296/300\n",
            "1563/1563 [==============================] - 76s 48ms/step - loss: 0.1171 - accuracy: 0.9844 - val_loss: 11.4567 - val_accuracy: 0.6752\n",
            "Epoch 297/300\n",
            "1563/1563 [==============================] - 76s 49ms/step - loss: 0.1046 - accuracy: 0.9852 - val_loss: 12.1168 - val_accuracy: 0.6719\n",
            "Epoch 298/300\n",
            "1563/1563 [==============================] - 76s 48ms/step - loss: 0.1078 - accuracy: 0.9848 - val_loss: 11.7600 - val_accuracy: 0.6737\n",
            "Epoch 299/300\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.0963 - accuracy: 0.9866 - val_loss: 11.5759 - val_accuracy: 0.6764\n",
            "Epoch 300/300\n",
            "1563/1563 [==============================] - 77s 49ms/step - loss: 0.1106 - accuracy: 0.9854 - val_loss: 11.7568 - val_accuracy: 0.6734\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "epochs = 300\n",
        "start_time = time.time()\n",
        "history = model.fit(x_train, y_train, epochs=epochs, validation_data=(x_test, y_test))\n",
        "end_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CD-fiE2pcFq0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97df05bd-0945-47b2-820b-83b3fe5885fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 4s 12ms/step - loss: 11.7568 - accuracy: 0.6734\n",
            "Training Time: 22285.848536491394 seconds\n",
            "Training Loss after 300 epochs: 0.11062169820070267\n",
            "Test Accuracy after 300 epochs: 0.6733999848365784\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "\n",
        "# Print results\n",
        "print(f\"Training Time: {end_time - start_time} seconds\")\n",
        "print(f\"Training Loss after {epochs} epochs: {history.history['loss'][-1]}\")\n",
        "print(f\"Test Accuracy after {epochs} epochs: {test_acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZle2-DPcIJ7"
      },
      "outputs": [],
      "source": [
        "#Problem 1B\n",
        "# Build extended CNN model\n",
        "extended_model = models.Sequential()\n",
        "extended_model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "extended_model.add(layers.MaxPooling2D((2, 2)))\n",
        "extended_model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "extended_model.add(layers.MaxPooling2D((2, 2)))\n",
        "extended_model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "# Additional layer\n",
        "extended_model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "extended_model.add(layers.MaxPooling2D((2, 2)))\n",
        "extended_model.add(layers.Flatten())\n",
        "extended_model.add(layers.Dense(128, activation='relu'))\n",
        "extended_model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the extended model\n",
        "extended_model.compile(optimizer='adam',\n",
        "                       loss='categorical_crossentropy',\n",
        "                       metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the extended model for 300 epochs\n",
        "start_time = time.time()\n",
        "extended_model.fit(x_train, y_train, epochs=300, validation_data=(x_test, y_test))\n",
        "training_time = time.time() - start_time"
      ],
      "metadata": {
        "id": "wY8E6FksgQCt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "55b78f30-2c3a-4e4c-b5ff-910f40889aa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-6ec8b272de49>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the extended model for 300 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mextended_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtraining_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1781\u001b[0m                         ):\n\u001b[1;32m   1782\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1783\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1784\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[0;31m# no_variable_creation function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m         return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1262\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1263\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mflat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mflat_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;34m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    253\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1479\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1480\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     ]\n\u001b[0;32m---> 60\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     61\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Problem 2\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "fZVZYBgjsu5M"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CIFAR-10 dataset and define data loaders\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzuMXeGshhxy",
        "outputId": "081ec313-3fc7-4d97-cb14-236265a6964e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 170498071/170498071 [00:08<00:00, 20922375.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#In class edition Defining th Module\n",
        "class ResNet10(nn.Module):\n",
        "    def __init__(self, n_chans1=32):\n",
        "        super().__init__()\n",
        "        self.n_chans1 = n_chans1\n",
        "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
        "        self.conv1_dropout = nn.Dropout2d(p=0.4)\n",
        "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(n_chans1 // 2, n_chans1 // 2, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(4 * 4 * n_chans1 // 2, 32)\n",
        "        self.fc2 = nn.Linear(32, 10)  # 10 classes for CIFAR-10\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
        "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
        "        out1 = out\n",
        "        out = F.max_pool2d(torch.relu(self.conv3(out)) + out1, 2)\n",
        "        out = out.view(-1, 4 * 4 * self.n_chans1 // 2)\n",
        "        out = torch.relu(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "t0Rm9Z5SjOqo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model = ResNet10().to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)"
      ],
      "metadata": {
        "id": "G_xH9RcumZil"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "epochs = 300\n",
        "def training_loop_reg(n_epochs, optimizer, model, loss_fn, train_loader, val_loader):\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss_train = 0.0\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs = imgs.to(device=device)\n",
        "            labels = labels.to(device=device)\n",
        "            outputs = model(imgs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "\n",
        "            # L2 regularization\n",
        "            l2_lambda = 0.001\n",
        "            l2_norm = sum(p.pow(2.0).sum() for name, p in model.named_parameters() if 'weight' in name)\n",
        "            loss = loss + l2_lambda * l2_norm\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_train += loss.item()\n",
        "        if epoch == 1 or epoch % 5 == 0:\n",
        "            print('{} Epoch {}, Training loss: {}'.format(datetime.datetime.now(), epoch, loss_train / len(train_loader)))\n",
        "\n",
        "        # Evaluation phase\n",
        "        if epoch % 5 == 0:  # You can adjust the frequency of evaluation\n",
        "            model.eval()\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            with torch.no_grad():\n",
        "                for val_imgs, val_labels in val_loader:\n",
        "                    val_imgs = val_imgs.to(device=device)\n",
        "                    val_labels = val_labels.to(device=device)\n",
        "                    val_outputs = model(val_imgs)\n",
        "                    _, predicted = torch.max(val_outputs, 1)\n",
        "                    total += val_labels.size(0)\n",
        "                    correct += (predicted == val_labels).sum().item()\n",
        "\n",
        "            accuracy = correct / total\n",
        "            print('{} Epoch {}, Validation Accuracy: {}'.format(datetime.datetime.now(), epoch, accuracy))\n",
        "  # Get evaluation accuracy at the end\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for val_imgs, val_labels in val_loader:\n",
        "            val_imgs = val_imgs.to(device=device)\n",
        "            val_labels = val_labels.to(device=device)\n",
        "            val_outputs = model(val_imgs)\n",
        "            _, predicted = torch.max(val_outputs, 1)\n",
        "            total += val_labels.size(0)\n",
        "            correct += (predicted == val_labels).sum().item()\n",
        "\n",
        "    final_accuracy = correct / total\n",
        "    print('Training finished. Final Evaluation Accuracy: {}'.format(final_accuracy))"
      ],
      "metadata": {
        "id": "W-QovuzMmciZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the training loop\n",
        "training_loop_reg(epochs, optimizer, model, loss_fn, trainloader, testloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aO6MGf8lrILF",
        "outputId": "5101156d-552a-4835-8aaa-706e564744e4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-12 22:36:20.214757 Epoch 1, Training loss: 2.3253070222752172\n",
            "2023-12-12 22:37:54.022886 Epoch 5, Training loss: 1.7044303339460622\n",
            "2023-12-12 22:37:59.074974 Epoch 5, Validation Accuracy: 0.4108\n",
            "2023-12-12 22:40:00.574102 Epoch 10, Training loss: 1.4944721629552524\n",
            "2023-12-12 22:40:04.517320 Epoch 10, Validation Accuracy: 0.4788\n",
            "2023-12-12 22:42:01.338265 Epoch 15, Training loss: 1.3644003807888616\n",
            "2023-12-12 22:42:05.185264 Epoch 15, Validation Accuracy: 0.5375\n",
            "2023-12-12 22:44:02.217979 Epoch 20, Training loss: 1.2652982959661947\n",
            "2023-12-12 22:44:06.666514 Epoch 20, Validation Accuracy: 0.5651\n",
            "2023-12-12 22:46:05.002390 Epoch 25, Training loss: 1.212263840391203\n",
            "2023-12-12 22:46:10.140880 Epoch 25, Validation Accuracy: 0.5934\n",
            "2023-12-12 22:48:09.215808 Epoch 30, Training loss: 1.1678725481795533\n",
            "2023-12-12 22:48:13.296908 Epoch 30, Validation Accuracy: 0.6172\n",
            "2023-12-12 22:50:10.580078 Epoch 35, Training loss: 1.1379103059201594\n",
            "2023-12-12 22:50:14.494315 Epoch 35, Validation Accuracy: 0.6295\n",
            "2023-12-12 22:52:13.908658 Epoch 40, Training loss: 1.108057443366941\n",
            "2023-12-12 22:52:18.897597 Epoch 40, Validation Accuracy: 0.646\n",
            "2023-12-12 22:54:16.637414 Epoch 45, Training loss: 1.0857800124094004\n",
            "2023-12-12 22:54:20.708852 Epoch 45, Validation Accuracy: 0.6535\n",
            "2023-12-12 22:56:22.898600 Epoch 50, Training loss: 1.0640843544927094\n",
            "2023-12-12 22:56:28.478035 Epoch 50, Validation Accuracy: 0.6539\n",
            "2023-12-12 22:58:23.343692 Epoch 55, Training loss: 1.0510586427758113\n",
            "2023-12-12 22:58:27.641000 Epoch 55, Validation Accuracy: 0.6692\n",
            "2023-12-12 23:00:22.706227 Epoch 60, Training loss: 1.0386646351850857\n",
            "2023-12-12 23:00:26.528257 Epoch 60, Validation Accuracy: 0.6743\n",
            "2023-12-12 23:02:21.863113 Epoch 65, Training loss: 1.0238283941203066\n",
            "2023-12-12 23:02:25.736452 Epoch 65, Validation Accuracy: 0.6704\n",
            "2023-12-12 23:04:19.046594 Epoch 70, Training loss: 1.0112024654665261\n",
            "2023-12-12 23:04:22.777992 Epoch 70, Validation Accuracy: 0.6779\n",
            "2023-12-12 23:06:14.651432 Epoch 75, Training loss: 1.00585512325282\n",
            "2023-12-12 23:06:18.533687 Epoch 75, Validation Accuracy: 0.6913\n",
            "2023-12-12 23:08:14.183465 Epoch 80, Training loss: 0.9963191952699285\n",
            "2023-12-12 23:08:18.170398 Epoch 80, Validation Accuracy: 0.6897\n",
            "2023-12-12 23:10:13.267323 Epoch 85, Training loss: 0.9830170053502788\n",
            "2023-12-12 23:10:17.661440 Epoch 85, Validation Accuracy: 0.6935\n",
            "2023-12-12 23:12:10.642400 Epoch 90, Training loss: 0.9866308543230872\n",
            "2023-12-12 23:12:15.572394 Epoch 90, Validation Accuracy: 0.6927\n",
            "2023-12-12 23:14:15.148292 Epoch 95, Training loss: 0.9784441983608334\n",
            "2023-12-12 23:14:19.493925 Epoch 95, Validation Accuracy: 0.6958\n",
            "2023-12-12 23:16:22.536387 Epoch 100, Training loss: 0.9694741511588816\n",
            "2023-12-12 23:16:26.531640 Epoch 100, Validation Accuracy: 0.7031\n",
            "2023-12-12 23:18:28.457401 Epoch 105, Training loss: 0.9643895473626568\n",
            "2023-12-12 23:18:33.374404 Epoch 105, Validation Accuracy: 0.7097\n",
            "2023-12-12 23:20:34.149019 Epoch 110, Training loss: 0.9614745377732055\n",
            "2023-12-12 23:20:38.249844 Epoch 110, Validation Accuracy: 0.7044\n",
            "2023-12-12 23:22:36.810930 Epoch 115, Training loss: 0.9623819473759293\n",
            "2023-12-12 23:22:42.762504 Epoch 115, Validation Accuracy: 0.7017\n",
            "2023-12-12 23:24:43.184258 Epoch 120, Training loss: 0.953400124674258\n",
            "2023-12-12 23:24:47.138132 Epoch 120, Validation Accuracy: 0.712\n",
            "2023-12-12 23:26:46.921852 Epoch 125, Training loss: 0.9563120036478847\n",
            "2023-12-12 23:26:52.808980 Epoch 125, Validation Accuracy: 0.7106\n",
            "2023-12-12 23:28:54.714684 Epoch 130, Training loss: 0.9541714755470491\n",
            "2023-12-12 23:28:58.799736 Epoch 130, Validation Accuracy: 0.6908\n",
            "2023-12-12 23:31:01.805204 Epoch 135, Training loss: 0.9447070853332119\n",
            "2023-12-12 23:31:06.768060 Epoch 135, Validation Accuracy: 0.7105\n",
            "2023-12-12 23:33:00.288979 Epoch 140, Training loss: 0.9427066555870768\n",
            "2023-12-12 23:33:04.963996 Epoch 140, Validation Accuracy: 0.697\n",
            "2023-12-12 23:34:57.706740 Epoch 145, Training loss: 0.9445576127380362\n",
            "2023-12-12 23:35:02.151669 Epoch 145, Validation Accuracy: 0.7021\n",
            "2023-12-12 23:36:53.384638 Epoch 150, Training loss: 0.9402713883868263\n",
            "2023-12-12 23:36:58.471277 Epoch 150, Validation Accuracy: 0.6993\n",
            "2023-12-12 23:38:54.386527 Epoch 155, Training loss: 0.9400516772056784\n",
            "2023-12-12 23:38:59.837224 Epoch 155, Validation Accuracy: 0.7126\n",
            "2023-12-12 23:40:54.309044 Epoch 160, Training loss: 0.9338140851243988\n",
            "2023-12-12 23:40:59.303408 Epoch 160, Validation Accuracy: 0.7145\n",
            "2023-12-12 23:42:53.715130 Epoch 165, Training loss: 0.9342205390296019\n",
            "2023-12-12 23:42:57.948546 Epoch 165, Validation Accuracy: 0.702\n",
            "2023-12-12 23:44:51.693758 Epoch 170, Training loss: 0.932439075406555\n",
            "2023-12-12 23:44:55.520011 Epoch 170, Validation Accuracy: 0.7182\n",
            "2023-12-12 23:46:49.419500 Epoch 175, Training loss: 0.9289502957287956\n",
            "2023-12-12 23:46:53.269890 Epoch 175, Validation Accuracy: 0.7119\n",
            "2023-12-12 23:48:48.477392 Epoch 180, Training loss: 0.9265631403002288\n",
            "2023-12-12 23:48:52.264152 Epoch 180, Validation Accuracy: 0.7256\n",
            "2023-12-12 23:50:47.675961 Epoch 185, Training loss: 0.9277894001482697\n",
            "2023-12-12 23:50:51.557929 Epoch 185, Validation Accuracy: 0.7121\n",
            "2023-12-12 23:52:47.240437 Epoch 190, Training loss: 0.9222884674358856\n",
            "2023-12-12 23:52:51.128379 Epoch 190, Validation Accuracy: 0.7249\n",
            "2023-12-12 23:54:47.578858 Epoch 195, Training loss: 0.9218933855176277\n",
            "2023-12-12 23:54:53.093871 Epoch 195, Validation Accuracy: 0.722\n",
            "2023-12-12 23:56:47.996232 Epoch 200, Training loss: 0.9206496639477323\n",
            "2023-12-12 23:56:52.749204 Epoch 200, Validation Accuracy: 0.712\n",
            "2023-12-12 23:58:47.308086 Epoch 205, Training loss: 0.9231113748782126\n",
            "2023-12-12 23:58:51.345534 Epoch 205, Validation Accuracy: 0.7225\n",
            "2023-12-13 00:00:46.849002 Epoch 210, Training loss: 0.9230154859440406\n",
            "2023-12-13 00:00:50.714954 Epoch 210, Validation Accuracy: 0.7207\n",
            "2023-12-13 00:02:45.991779 Epoch 215, Training loss: 0.9164859919291933\n",
            "2023-12-13 00:02:49.789283 Epoch 215, Validation Accuracy: 0.7104\n",
            "2023-12-13 00:04:44.608928 Epoch 220, Training loss: 0.9173104942149823\n",
            "2023-12-13 00:04:48.376524 Epoch 220, Validation Accuracy: 0.7188\n",
            "2023-12-13 00:06:43.084941 Epoch 225, Training loss: 0.9138573839536408\n",
            "2023-12-13 00:06:47.659484 Epoch 225, Validation Accuracy: 0.7267\n",
            "2023-12-13 00:08:41.770822 Epoch 230, Training loss: 0.910655396826127\n",
            "2023-12-13 00:08:47.333437 Epoch 230, Validation Accuracy: 0.7299\n",
            "2023-12-13 00:10:40.311174 Epoch 235, Training loss: 0.910004312196351\n",
            "2023-12-13 00:10:45.756057 Epoch 235, Validation Accuracy: 0.7226\n",
            "2023-12-13 00:12:40.430658 Epoch 240, Training loss: 0.9171989307074291\n",
            "2023-12-13 00:12:44.629508 Epoch 240, Validation Accuracy: 0.7344\n",
            "2023-12-13 00:14:38.828905 Epoch 245, Training loss: 0.9104728835165653\n",
            "2023-12-13 00:14:42.706957 Epoch 245, Validation Accuracy: 0.7321\n",
            "2023-12-13 00:16:37.529212 Epoch 250, Training loss: 0.9098969518071245\n",
            "2023-12-13 00:16:41.364721 Epoch 250, Validation Accuracy: 0.7165\n",
            "2023-12-13 00:18:36.198702 Epoch 255, Training loss: 0.9124603208983341\n",
            "2023-12-13 00:18:40.014843 Epoch 255, Validation Accuracy: 0.7174\n",
            "2023-12-13 00:20:34.503260 Epoch 260, Training loss: 0.9089852663714563\n",
            "2023-12-13 00:20:38.493446 Epoch 260, Validation Accuracy: 0.7186\n",
            "2023-12-13 00:22:32.672584 Epoch 265, Training loss: 0.9076678092065065\n",
            "2023-12-13 00:22:38.195392 Epoch 265, Validation Accuracy: 0.7274\n",
            "2023-12-13 00:24:36.669384 Epoch 270, Training loss: 0.9090835477232628\n",
            "2023-12-13 00:24:41.609223 Epoch 270, Validation Accuracy: 0.7285\n",
            "2023-12-13 00:26:40.543800 Epoch 275, Training loss: 0.904854636134394\n",
            "2023-12-13 00:26:44.606078 Epoch 275, Validation Accuracy: 0.7022\n",
            "2023-12-13 00:28:45.231077 Epoch 280, Training loss: 0.9048984941009366\n",
            "2023-12-13 00:28:49.257482 Epoch 280, Validation Accuracy: 0.7275\n",
            "2023-12-13 00:30:51.020705 Epoch 285, Training loss: 0.905213849547574\n",
            "2023-12-13 00:30:56.781119 Epoch 285, Validation Accuracy: 0.7194\n",
            "2023-12-13 00:32:53.905465 Epoch 290, Training loss: 0.9045827659347173\n",
            "2023-12-13 00:32:59.008726 Epoch 290, Validation Accuracy: 0.7251\n",
            "2023-12-13 00:34:52.762942 Epoch 295, Training loss: 0.9076818636311289\n",
            "2023-12-13 00:34:57.198455 Epoch 295, Validation Accuracy: 0.7286\n",
            "2023-12-13 00:36:51.632028 Epoch 300, Training loss: 0.9022127575124316\n",
            "2023-12-13 00:36:55.429982 Epoch 300, Validation Accuracy: 0.7198\n",
            "Training finished. Final Evaluation Accuracy: 0.7164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Problem 2B\n",
        "# Define the model with Weight Decay, Dropout, and Batch Normalization\n",
        "class NetRes2B(nn.Module):\n",
        "    def __init__(self, n_chans1=32, dropout_prob=0.3):\n",
        "        super().__init__()\n",
        "        self.n_chans1 = n_chans1\n",
        "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
        "        self.conv1_bn = nn.BatchNorm2d(n_chans1)\n",
        "        self.conv1_dropout = nn.Dropout2d(p=dropout_prob)\n",
        "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
        "        self.conv2_bn = nn.BatchNorm2d(n_chans1 // 2)\n",
        "        self.conv3 = nn.Conv2d(n_chans1 // 2, n_chans1 // 2, kernel_size=3, padding=1)\n",
        "        self.conv3_bn = nn.BatchNorm2d(n_chans1 // 2)\n",
        "        self.fc1 = nn.Linear(4 * 4 * n_chans1 // 2, 32)\n",
        "        self.fc2 = nn.Linear(32, 10)  # 10 classes for CIFAR-10\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.relu(self.conv1_bn(self.conv1(x))), 2)\n",
        "        out = F.max_pool2d(torch.relu(self.conv2_bn(self.conv2(out))), 2)\n",
        "        out1 = out\n",
        "        out = F.max_pool2d(torch.relu(self.conv3_bn(self.conv3(out))) + out1, 2)\n",
        "        out = out.view(-1, 4 * 4 * self.n_chans1 // 2)\n",
        "        out = torch.relu(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "_yiZAyDPuZTW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model with Weight Decay, Dropout, and Batch Normalization\n",
        "epochs = 300\n",
        "model2B = NetRes2B(dropout_prob=0.3).to(device)\n",
        "loss_fn2B = nn.CrossEntropyLoss()\n",
        "optimizer2B = optim.SGD(model2B.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
      ],
      "metadata": {
        "id": "uPbz29KBu0Mq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training with Weight Decay:\")\n",
        "training_loop_reg(epochs, optimizer2B, model2B, loss_fn2B, trainloader, testloader)"
      ],
      "metadata": {
        "id": "oJLJfRoHvCs4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "20bb4467-c0f5-48b2-dd01-3de681ff2603"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with Weight Decay:\n",
            "2023-12-13 00:41:56.066238 Epoch 1, Training loss: 1.872602863232498\n",
            "2023-12-13 00:43:27.668191 Epoch 5, Training loss: 1.3076506555842622\n",
            "2023-12-13 00:43:31.496421 Epoch 5, Validation Accuracy: 0.584\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-6e522cb747bb>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training with Weight Decay:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtraining_loop_reg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer2B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel2B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn2B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-6787e8d63ddd>\u001b[0m in \u001b[0;36mtraining_loop_reg\u001b[0;34m(n_epochs, optimizer, model, loss_fn, train_loader, val_loader)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mloss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1328\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1329\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1292\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1294\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1295\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrebuild_storage_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m     \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m'''Get the fd.  This should only be called once.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_resource_sharer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mget_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mident\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_writable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/reduction.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(cls, obj, protocol)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/reduction.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyreg_dispatch_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extra_reducers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2b = NetRes2B(dropout_prob=0.3).to(device)\n",
        "\n",
        "# Loss function and optimizer with Dropout\n",
        "loss_fn2b = nn.CrossEntropyLoss()\n",
        "optimizer2b = optim.SGD(model2b.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "QAsHwtJ4vJpe"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "print(\"\\nTraining with Batch Normalization:\")\n",
        "training_loop_reg(epochs, optimizer2b, model2b, loss_fn2b, trainloader, testloader)"
      ],
      "metadata": {
        "id": "TzphFdfGvfLw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "615fec9d-30d1-4d72-aa78-43c694879f41"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with Batch Normalization:\n",
            "2023-12-13 00:39:40.087138 Epoch 1, Training loss: 1.922074217015825\n",
            "2023-12-13 00:41:13.853167 Epoch 5, Training loss: 1.3011005293682714\n",
            "2023-12-13 00:41:17.777779 Epoch 5, Validation Accuracy: 0.5914\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-d7e4aebbfcec>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTraining with Batch Normalization:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtraining_loop_reg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer2b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel2b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn2b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-6787e8d63ddd>\u001b[0m in \u001b[0;36mtraining_loop_reg\u001b[0;34m(n_epochs, optimizer, model, loss_fn, train_loader, val_loader)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-c9153a3bfe8b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_chans1\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}